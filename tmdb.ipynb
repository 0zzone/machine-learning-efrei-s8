{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4afcc98",
   "metadata": {},
   "source": [
    "# 🍿 Final Project - The Movie Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d80b43",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad6b1d8",
   "metadata": {},
   "source": [
    "Is money strong enough to determine profitability, popularity and public appreciation of movies?  \n",
    "Are movies so plain that one may guess their genres with only their overview? \n",
    "\n",
    "For this project, we have targeted movies to analyze them and predict:\n",
    "- A budget a movie must be allocated, to achieve a certain amount of revenue, public appreciation and popularity (Numerical);\n",
    "- And the single or multiple genres of a movie, by providing its text overview (Textual).\n",
    "\n",
    "This project uses the TMDB API but is not endorsed or certified by TMDB.\n",
    "\n",
    "TODOs:\n",
    "- TODO: more models \n",
    "- TODO: comparison of the models with visualization \n",
    "- TODO: conclusion - business recommendations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bb2a61",
   "metadata": {},
   "source": [
    "Imports we will be using for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c85c5596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import spacy\n",
    "import spacy.cli\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3d5537c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For BERT embeddings\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eac5705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fr-core-news-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_md-3.8.0/fr_core_news_md-3.8.0-py3-none-any.whl (45.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fr_core_news_md')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Download the language model, that includes tokenization, part-of-speech tagging, and lemmatization\n",
    "SPACY_MODEL=\"fr_core_news_md\"\n",
    "spacy.cli.download(SPACY_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8267692",
   "metadata": {},
   "source": [
    "## How did we collect the data?\n",
    "> The Movie Database (TMDB) is a public API which provides lot of data about movies, series and TV shows, including descriptions, title, ratings, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f08a8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LANGUAGE=\"fr-FR\"\n",
    "API_KEY = input(\"Enter your API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddd6446",
   "metadata": {},
   "source": [
    "For safety, we will verify whether the entered API_KEY is correct.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29fb8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Throw error if API_KEY is incorrect\n",
    "try:\n",
    "    if not API_KEY:\n",
    "        raise ValueError(\"no API_KEY provided\")\n",
    "    url = \"https://api.themoviedb.org/3/authentication\"\n",
    "    headers = {\"accept\": \"application/json\", \"Authorization\": f\"Bearer {API_KEY}\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "except Exception as e:\n",
    "    raise ValueError(\n",
    "        \"API_KEY is not set or invalid. Please provide a valid API key.\"\n",
    "    ) from e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19344008",
   "metadata": {},
   "source": [
    "We need to get the IDs of the movies we'd like to collect.  \n",
    "\n",
    "TMDB uses a system of pages, where each page contains a list of movies.  \n",
    "We will fetch the data with API requests in French. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be919fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_array(page):\n",
    "    try:\n",
    "        url = f\"https://api.themoviedb.org/3/discover/movie?language={DATA_LANGUAGE}&page={page}\"\n",
    "        headers = {\n",
    "            \"accept\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {API_KEY}\"\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, headers=headers).text\n",
    "        data = json.loads(response)\n",
    "        return [movie['id'] for movie in data['results']]\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "IDs = []\n",
    "for page in tqdm(range(1, 501)):\n",
    "    IDs += get_full_array(page)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19497c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 10 movie IDs\n",
    "IDs[:10] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2620fae",
   "metadata": {},
   "source": [
    "After we have collected the movies's ID, we can now get the details of each one in French. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d57d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for id in tqdm(range(len(IDs))):\n",
    "    id = IDs[id]\n",
    "    url = f\"https://api.themoviedb.org/3/movie/{id}?language={DATA_LANGUAGE}\"\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers).text\n",
    "    movie = json.loads(response)\n",
    "    data.append(movie)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee01b59",
   "metadata": {},
   "source": [
    "We convert the array into `Pandas` dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b2adff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "pd.set_option('display.max_columns', None) # To display all columns\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac584747",
   "metadata": {},
   "source": [
    "Finally, we filter the irrelevant rows out,  \n",
    "and we save the dataframe to a permanent file, to avoid refetching again.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7519e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.drop(df[df.overview == \"\"].index)  # Drop rows with empty values in 'overview' (meaning they have no French overview)\n",
    "df_copy.to_pickle(\"all_data_fr.pkl\") # Save DataFrame to a pickle file (To directly use the DataFrame later, instead of re-fetching)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e99ccf",
   "metadata": {},
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb04225",
   "metadata": {},
   "source": [
    "### Importing and filtering data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8ccb4f",
   "metadata": {},
   "source": [
    "In case we already have fetched the data, we can load the dataframe using the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a018bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df: pd.DataFrame = pd.read_pickle(\"all_data_fr.pkl\")\n",
    "fr_data = df.copy()\n",
    "fr_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e05d9f6",
   "metadata": {},
   "source": [
    "### Numerical values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0c1a16",
   "metadata": {},
   "source": [
    "Let's show the plots related to the collected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b87715",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2)\n",
    "\n",
    "ax[0][0].scatter(fr_data['vote_average'], fr_data['budget'])\n",
    "ax[0][0].set_xlabel('Vote Average')\n",
    "ax[0][0].set_ylabel('Budget')\n",
    "\n",
    "ax[0][1].scatter(fr_data['popularity'], fr_data['budget'])\n",
    "ax[0][1].set_xlabel('Popularity')\n",
    "ax[0][1].set_ylabel('Budget')\n",
    "\n",
    "ax[1][0].scatter(fr_data['revenue'], fr_data['budget'])\n",
    "ax[1][0].set_xlabel('Revenue')\n",
    "ax[1][0].set_ylabel('Budget')\n",
    "\n",
    "ax[1][1].axis('off')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef768335",
   "metadata": {},
   "source": [
    "For the numerical values, we remove null values. Then, we split the dataset into train & test datasets.\n",
    "\n",
    "Finally, we normalize the X train dataset, and we use the obtained values of the normalization for the X test dataset.  \n",
    "We do not need to normalize the y datasets, because they are predicted. \n",
    "\n",
    "We also do not filter out the extreme values (outliers), because they are relevant in our study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd9c7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_num_df = fr_data[(fr_data['revenue'] > 0) & (fr_data['budget'] > 0)] # Some movies do not have revenue or budget data\n",
    "print(\"filtered df shape: \", filtered_num_df.shape)\n",
    "numerical_num_df = filtered_num_df[[\"budget\", \"vote_average\", \"revenue\", \"popularity\"]] # Extracting numerical columns\n",
    "\n",
    "X = numerical_num_df[['revenue', 'vote_average', 'popularity']]\n",
    "y = numerical_num_df['budget']\n",
    "\n",
    "X_train_num, X_test_num, y_train_num, y_test_num = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "num_scaler = StandardScaler()\n",
    "X_train_num_scaled = num_scaler.fit_transform(X_train_num)\n",
    "X_test_num_scaled = num_scaler.transform(X_test_num)\n",
    "\n",
    "X_train_num_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c37a5c9",
   "metadata": {},
   "source": [
    "### Textual values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7c4e7d",
   "metadata": {},
   "source": [
    "For the textual part, we have analyzed it as a multi-labelling problem, which means that a movie's overview can belong to several `genres`.  \n",
    "This is a multi-label problem, because a movie can be associated with multiple genres. \n",
    "\n",
    "Furthermore, we will preprocess the text: convert the text to lowercase, remove punctuation and lemmatize the words.  \n",
    "We can preprocess before the train & test split, because there is no risk of data leakage.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14550d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = fr_data[['overview', 'genres']]\n",
    "\n",
    "# Convert genres from a list of dictionaries to a list of strings\n",
    "df_text.loc[:, 'genres'] = df_text['genres'].apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])\n",
    "\n",
    "# Preprocess the overview text\n",
    "nlp = spacy.load(SPACY_MODEL)\n",
    "with nlp.select_pipes(disable=[]):  # We use the parser component (syntaxic analysis), tokenization (text -> list), lemmatization (word to most basic form) and NER (unlinkable words) \n",
    "    preprocessed_texts = []\n",
    "    for (i, doc) in enumerate(nlp.pipe(df_text[\"overview\"], batch_size=50, n_process=-1)):\n",
    "        # Exclude tokens that are part of a named entity (NER), punctuation, and stop words\n",
    "        preprocessed_texts.append(' '.join([token.lemma_ for token in doc if not token.ent_type_ and not token.is_punct and not token.is_stop]))\n",
    "    df_text.loc[:, \"overview\"] = preprocessed_texts\n",
    "    \n",
    "df_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c2d635",
   "metadata": {},
   "source": [
    "For the classification, the model needs all the target possibilities. Therefore, we will list all the movie genres into one place. \n",
    "\n",
    "Classes are the possible categories of the dataset.  \n",
    "Labels are the actual categories assigned to the data samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0009cc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genres_set = set()\n",
    "for genres in df_text['genres']:\n",
    "    all_genres_set.update(genres)\n",
    "all_genres: list[str] = sorted(list(all_genres_set))\n",
    "all_genres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a59a2f7",
   "metadata": {},
   "source": [
    "We will train the models with multi-label binarizer, as the teacher recommended. \n",
    "\n",
    "It transforms the labels into numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2856373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer(classes=all_genres)\n",
    "y_text = mlb.fit_transform(df_text['genres']) # We embed the target\n",
    "\n",
    "X_train_text, X_test_text, y_train_text, y_test_text = train_test_split(\n",
    "    df_text['overview'],\n",
    "    y_text,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "y_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae46367",
   "metadata": {},
   "source": [
    "When we display the class distribution of the dataset, we can observe it is skewed towards \"drama\".  \n",
    "We will need to handle this problem later.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daccf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(all_genres, height=np.sum(y_text, axis=0))\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Distribution of genre labels\")\n",
    "plt.xlabel(\"Genres\")\n",
    "plt.ylabel(\"Number of Samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f198f44e",
   "metadata": {},
   "source": [
    "Let's display a word cloud for every movie genre.\n",
    "\n",
    "We may notice that some words have high correlation with their genre:\n",
    "- \"documentaire\" word is highly present in \"documentaire\" genre;\n",
    "- \"soldat\" and \"guerre\" words are overwhelmingly present in data samples with \"guerre\" genre;\n",
    "- \"musique\" with \"musique\".\n",
    "\n",
    "Later on, this would definitely help the label prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d71ac73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "def display_wordclouds(n_cols=4):\n",
    "    n_rows = (len(all_genres) + n_cols - 1) // n_cols\n",
    "    idx = 0\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for label in all_genres:\n",
    "        # Get all overviews for this label\n",
    "        text = \" \".join(\n",
    "            df_text[df_text[\"genres\"].apply(lambda genres: label in genres)][\"overview\"]\n",
    "        )\n",
    "        wordcloud = WordCloud(background_color=\"white\").generate(text)\n",
    "        axes[idx].imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        axes[idx].set_title(label)\n",
    "        axes[idx].axis(\"off\")\n",
    "        idx += 1\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for j in range(idx, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "display_wordclouds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc97c5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the length of each overview in words\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(df_text['overview'].apply(lambda x: len(x.split())), bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Histogram of Text Sample Lengths')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eda6377",
   "metadata": {},
   "source": [
    "## Implementation of models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf744f9",
   "metadata": {},
   "source": [
    "### Numerical values \n",
    "\n",
    "For the numerical values, we are going to use a Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4144fe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"Linear Regression\", \"Decision Tree Regressor\", \"Gradient Boosting Regressor\", \"Random Forest Regressor\"]\n",
    "values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5459d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model_num = LinearRegression()\n",
    "lr_model_num.fit(X_train_num_scaled, y_train_num)\n",
    "\n",
    "y_pred_num = lr_model_num.predict(X_test_num_scaled)\n",
    "\n",
    "r2 = r2_score(y_test_num, y_pred_num)\n",
    "values.append(r2)\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test_num, y_pred_num))\n",
    "print(\"R² Score:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058cfb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "regressor = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "regressor.fit(X_train_num_scaled, y_train_num)\n",
    "y_pred_num_tree = regressor.predict(X_test_num_scaled)\n",
    "\n",
    "r2 = r2_score(y_test_num, y_pred_num_tree)\n",
    "values.append(r2)\n",
    "print(\"Mean Squared Error of the Decision Tree Regressor:\", mean_squared_error(y_test_num, y_pred_num_tree))\n",
    "print(\"R² Score:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966232e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 10, 20, 30, None],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='r2',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_num_scaled, y_train_num)\n",
    "\n",
    "y_pred = grid_search.predict(X_test_num_scaled)\n",
    "\n",
    "grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093e2c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "values.append(grid_search.best_score_)\n",
    "print(\"Best R2: \", grid_search.best_score_)\n",
    "print(\"Best parameters: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d354ed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rf_model_num  = RandomForestRegressor(random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'max_depth': [None, 10, 20, 30],  \n",
    "    'min_samples_split': [2, 5, 10],  \n",
    "    'min_samples_leaf': [1, 2, 4] \n",
    "}\n",
    "\n",
    "scoring = 'r2'\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_model_num ,\n",
    "    param_grid=param_grid,\n",
    "    scoring=scoring,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_num_scaled, y_train_num)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "# grid_search\n",
    "\n",
    "y_pred_rf_num = grid_search.predict(X_test_num_scaled)\n",
    "\n",
    "r2 = r2_score(y_test_num, y_pred_rf_num)\n",
    "values.append(r2)\n",
    "print(\"Random Forest Mean Squared Error:\", mean_squared_error(y_test_num, y_pred_rf_num))\n",
    "print(\"Random Forest R² Score:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4384c6c",
   "metadata": {},
   "source": [
    "### Textual values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e41926",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b478ca5",
   "metadata": {},
   "source": [
    "Vectorizing the text with TF-IDF, which was the best vectorizer in the second lab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5123f803",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words=None)\n",
    "X_train_text_tfidf = tfidf_vectorizer.fit_transform(X_train_text)\n",
    "\n",
    "# Display the first 5 rows of the TF-IDF vectors \n",
    "X_train_text_tfidf.toarray()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6025698d",
   "metadata": {},
   "source": [
    "We will use OneVsRestClassifier to solve our multi-label classification problem.  \n",
    "This works by training 1 classifier per class which learns to distinguish this class versus all others; and for prediction, for each class, the classifier with the highest score is chosen. \n",
    "\n",
    "To handle class imbalance, we also use a \"balanced\" class weight, which penalizes misclassification of minority genres more.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e267532",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Multi-label Logistic Regression on TF-IDF\n",
    "\n",
    "def MultiLabel_LR(X_train_text, X_test_text):\n",
    "    lr_model_text = OneVsRestClassifier(LogisticRegression(max_iter=1000, class_weight=\"balanced\"))\n",
    "    lr_model_text.fit(X_train_text, y_train_text)\n",
    "\n",
    "    y_pred_text = lr_model_text.predict(X_test_text)\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test_text, y_pred_text, target_names=mlb.classes_, zero_division=np.nan))\n",
    "\n",
    "\n",
    "MultiLabel_LR(X_train_text_tfidf, tfidf_vectorizer.transform(X_test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0234bb9d",
   "metadata": {},
   "source": [
    "After fitting the One Vs. Rest classifier, we now can estimate its effectiveness with the test dataset.  \n",
    "Note that the accuracy score is very low. Indeed, in multi-label classification, for a test sample, the set of its predicted labels must exactly match the corresponding set of its real labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a0ce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Naive Bayes on TF-IDF\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def naive_bayes(X_train_text, X_test_text):\n",
    "    nb_model_text = OneVsRestClassifier(MultinomialNB())\n",
    "    nb_model_text.fit(X_train_text, y_train_text)\n",
    "\n",
    "    y_pred_text_tfidf = nb_model_text.predict(X_test_text)\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test_text, y_pred_text_tfidf, target_names=mlb.classes_))\n",
    "\n",
    "naive_bayes(X_train_text_tfidf, tfidf_vectorizer.transform(X_test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54474db3",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8556175",
   "metadata": {},
   "source": [
    "#### BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee688d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(current_device)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "def get_embeddings(texts, tokenizer, model, batch_size=16, device=current_device):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\", max_length=256)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            embeddings.append(batch_embeddings)\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# Example: get embeddings for train and test sets\n",
    "X_train_embeddings = get_embeddings(X_train_text.tolist(), tokenizer, model)\n",
    "X_test_embeddings = get_embeddings(X_test_text.tolist(), tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0c1f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes on BERT unapplicable because of negative values.\n",
    "\n",
    "# Multi-label Logistic Regression on BERT embeddings\n",
    "MultiLabel_LR(X_train_embeddings, X_test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d0cf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(models, values)\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('R² Score')\n",
    "plt.title('Model Performance Comparison (Numerical Data)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799dc827",
   "metadata": {},
   "source": [
    "As we can see above, the Random Forest model is better than the others for different reasons:\n",
    "- Random Forest trains a set of decision trees with different subsets -> avoid overfitting\n",
    "- Random Forest captures non-linear problem (real life problem are never perfectly linear)\n",
    "- Random Forest computes the variance by the average of all the small trees (Decision Tree might has a huge variance) -> more stable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80261ab0",
   "metadata": {},
   "source": [
    "#### BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d549e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# We need to download the stop words for French (not provided by default in sklearn)\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "french_stop_words = stopwords.words('french')\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=french_stop_words)\n",
    "X_train_vectorized_bow = vectorizer.fit_transform(X_train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aef598",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Naive Bayes on BoW\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_model_text = OneVsRestClassifier(MultinomialNB())\n",
    "nb_model_text.fit(X_train_vectorized_bow, y_train_text)\n",
    "\n",
    "naive_bayes(X_train_vectorized_bow, vectorizer.transform(X_test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70ce23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiLabel_LR(X_train_vectorized_bow, vectorizer.transform(X_test_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
